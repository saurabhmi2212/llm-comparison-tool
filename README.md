
---

# **LLM Comparison Tool ğŸš€**  

*A Streamlit-based benchmarking dashboard for comparing LLMs like OpenAI GPT, Google Gemini, Cohere, and Anthropic Claude.*

![LLM Benchmarking](https://raw.githubusercontent.com/jitu028/llm-comparison-tool/main/assets/banner.png) 

---

## **ğŸ“Œ Overview**  

The **LLM Comparison Tool** allows users to **benchmark, analyze, and compare** various Large Language Models (LLMs) across **latency, accuracy, and cost per 1K tokens**.  

### **ğŸ¯ Key Features** 

âœ… Compare multiple LLMs from **OpenAI, Google Gemini, Cohere, and Anthropic**  
âœ… **Interactive UI** using **Streamlit** for seamless benchmarking  
âœ… **Performance Metrics:** Latency, cost, and accuracy visualization  
âœ… **User Feedback Collection** & Cloud Storage Logging  
âœ… **Fully Deployable on Cloud Run**  

---

## **ğŸ› ï¸ Project Structure**  
```bash
llm-comparison-tool/
â”‚â”€â”€ backend/                # Backend API
â”‚   â”œâ”€â”€ .gcloudignore
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ llm_benchmark_api.py   # Backend API script
â”‚   â”œâ”€â”€ requirements.txt
â”‚
â”‚â”€â”€ frontend/               # Streamlit Dashboard UI
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ llm_benchmark_dashboard.py  # UI script
â”‚   â”œâ”€â”€ requirements.txt
â”‚
â”‚â”€â”€ scripts/                # Deployment Scripts
â”‚   â”œâ”€â”€ deploy_llm_comparison_api.sh
â”‚   â”œâ”€â”€ deploy_llm_comparison_dashboard.sh
â”‚
â”‚â”€â”€ README.md
```

---

## **ğŸ“¦ Installation & Setup**  

### **ğŸ”¹ 1. Clone the Repository**
```bash
git clone https://github.com/saurabhmi2212/llm-comparison-tool
cd llm-comparison-tool
```

### **ğŸ”¹ 2. Install Dependencies**
#### Backend API:
```bash
cd backend
pip install -r requirements.txt
```
#### Frontend Dashboard:
```bash
cd frontend
pip install -r requirements.txt
```

### **ğŸ”¹ 3. Run Locally**
#### Run Backend API:
```bash
cd backend
python llm_benchmark_api.py
```
#### Run Frontend Dashboard:
```bash
cd frontend
streamlit run llm_benchmark_dashboard.py
```

---

## **ğŸš€ Deployment to Google Cloud Run**
You can deploy both **frontend (Streamlit UI)** and **backend (Flask API)** to **Cloud Run**.

### **ğŸ”¹ 1. Deploy Backend API**
Run the following deployment script:
```bash
./scripts/deploy_llm_comparison_api.sh
```
This will:
- Build & push the backend API Docker image
- Deploy it to **Google Cloud Run**

### **ğŸ”¹ 2. Deploy Frontend Dashboard**
Run the following deployment script:
```bash
./scripts/deploy_llm_comparison_dashboard.sh
```
This will:
- Build & push the frontend Streamlit UI Docker image
- Deploy it to **Google Cloud Run**

---

## **ğŸŒ API Endpoints**
| Endpoint              | Method | Description |
|----------------------|--------|-------------|
| `/benchmark`         | POST   | Run a benchmark for a model |
| `/update-feedback`   | POST   | Update user feedback |
| `/past-results`      | GET    | Retrieve past benchmark results |

Example **cURL request**:
```bash
curl -X POST "https://YOUR_BACKEND_URL/benchmark" \
     -H "Content-Type: application/json" \
     -d '{"model_name": "gemini-1.5-pro-001", "prompt": "What is Generative AI?"}'
```

---

## **ğŸ“Š Dashboard Features**
### **ğŸ”¹ LLM Benchmarking**
- Compare multiple models side-by-side  
- Measure **latency, accuracy, and cost per 1K tokens**  
- Interactive **bar & box plots**  

### **ğŸ”¹ User Feedback**
- **Rate model responses (1-5)**  
- Store & update feedback in **Google Cloud Storage**  

### **ğŸ”¹ Past Benchmark Results**
- View **previous benchmarking data**  
- Sort & filter by **model type, latency, and cost**  

---

## **ğŸ“œ Roadmap**

- [ ] **Add More LLM Providers** (Meta Llama, Mistral)  
- [ ] **Live Model Cost Tracking**  
- [ ] **Auto-generated Model Insights**  
- [ ] **Multi-user login for personalized tracking**  

---

## **ğŸ“© Contributions**

Contributions are **welcome**! Feel free to **fork, submit PRs, or open issues**.

---

## **ğŸ‘¤ Author**

 **Saurabh Mishra**

---

## **ğŸ“„ License**
This project is licensed under the **MIT License**.

---
